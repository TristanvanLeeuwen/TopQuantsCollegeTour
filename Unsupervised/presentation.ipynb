{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the black box of Machine Learning II\n",
    "\n",
    "Tristan van Leeuwen\n",
    "\n",
    "![](./figures/cube-250082_1280.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Given $m$ samples $x^{(i)} \\in \\mathbb{R}^n$, find a *simple* description / summary of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* construct a probability distribution $f(x)$\n",
    "* construct a reduced dataset $\\widetilde{X} = UV^T$, with $U \\in \\mathbb{R}^{n\\times p}$ and $V \\in \\mathbb{R}^{m\\times p}$ that requires $\\mathcal{O}(m + n)$ storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Density estimation\n",
    "\n",
    "![](./figures/ML.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Normal distribution\n",
    "\n",
    "Fit a normal distribution to the samples\n",
    "\n",
    "$$\\max_{\\mu,\\Sigma} \\prod_{i=1}^m(2\\pi)^{-n/2}\\text{det}(\\Sigma)^{-1/2}\\exp\\left(-\\frac{1}{2}(x^{(i)} - \\mu)^T\\Sigma^{-1}(x^{(i)} - \\mu)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "leads to\n",
    "$$\\min_{\\mu,\\Sigma}\\,\\, m\\log(|\\Sigma|) + \\sum_{i=1}^m(x^{(i)} - \\mu)^T\\Sigma^{-1}(x^{(i)} - \\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which gives\n",
    "$$\\widehat{\\mu} = \\frac{1}{m}\\sum_{i=1}^m x^{(i)}, \\quad \\widehat{\\Sigma} = \\frac{1}{m}\\sum_{i=1}^m \\left(x^{(i)} - \\widehat{\\mu}\\right)\\left(x^{(i)} - \\widehat{\\mu}\\right)^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/Gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gaussian mixture model\n",
    "\n",
    "Here, the goal is to fit a probability distribution of the form\n",
    "$$f(x) = \\sum_{i=1}^k w_i f_i(x),$$\n",
    "where $f_i(x)$ is a Normal distribution with paramaters $(\\mu_i, \\Sigma_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can be obtained by solving\n",
    "$$\\min_{\\mu, \\Sigma}  -\\log\\left(\\sum_{i=1}^k w_i f_i(x)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/Old_Faithful_Geyser_KDE_with_plugin_bandwidth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Sparse (inverse) covariance estimation\n",
    "\n",
    "Given a sample-covariance matrix $C$, find an approximation, $\\Sigma$, that has a *sparse inverse*:\n",
    "\n",
    "$$\\min_{\\Sigma \\succ 0} -\\log|\\Sigma| + \\text{trace}(C\\Sigma) + \\alpha \\|\\Sigma\\|_1.$$\n",
    "\n",
    "* used to detect network structure in data\n",
    "* large-scale algorithms available\n",
    "* has a closed-form solution for very specific cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "![](./figures/MnistExamples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Organize the data in a matrix $X = (x^{(1)}, x^{(2)}, \\ldots, x^{(m)})$\n",
    "\n",
    "* the matrix $C = XX^T$ is a sample-approximation of $\\mathbb{E}(x_i x_j)$\n",
    "* the matrix $C = X^TX$ gives information about the pairwise-distances between $x^{(i)}$ and $x^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "A matrix $X \\in \\mathbb{R}^{n \\times m}$ has a  *singular value decomposition*\n",
    "$$X = U S V^T,$$\n",
    "with $U \\in \\mathbb{R}^{n\\times n}$, $V \\in \\mathbb{R}^{m\\times m}$ are *unitary matrices* and $S$ is a diagonal matrix with non-negative entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Expressing each column of $X$ (sample) as linear combination of the colums of $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Approximate the matrix by leaving out the important components (small singular values)\n",
    "\n",
    "$$X \\approx \\widetilde{X} = U_p S_p V_p^T.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We can think of $\\widetilde{X}$ as the solution of\n",
    "$$\\min_{\\widetilde{X}} \\|X - \\widetilde{X}\\|_{F}^2 \\quad \\text{s.t.} \\quad \\text{rank}(\\widetilde{X}) \\leq p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/500px-Singular_value_decomposition_visualisation.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### All together now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $$\\left(\\begin{matrix} 1 & 2 & 3 \\\\ 0 & 0 & 0 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $$\\left(\\begin{matrix} 1 & 2 & 3 \\\\ 0 & 0 & 1 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $$\\left(\\begin{matrix} 1 & 2 & 6 \\\\ 1 & 1 & 4 \\\\ 1 & 2 & 6 \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Principle component analysis\n",
    "\n",
    "![](./figures/mnist_pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singular values\n",
    "\n",
    "![](./figures/mnist_sigma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Non-negative matrix factorization\n",
    "\n",
    "Decompose a matrix in non-negative components\n",
    "\n",
    "$$\\min_{U,V} \\|X - UV^T\\|_{F}^2, \\quad \\text{s.t.} \\quad u_{ij}, v_{ij} \\geq 0$$\n",
    "\n",
    "* used for clustering\n",
    "* positivity is important for some applications (images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Robust PCA\n",
    "\n",
    "$$\\min_{\\widetilde{X}} L(X - \\widetilde{X}) \\quad \\text{s.t.} \\quad \\text{rank}(\\widetilde{X}) \\leq p,$$\n",
    "\n",
    "where $L$ is a *robust* penalty (e.g., $L(R) = \\|R\\|_1$).\n",
    "\n",
    "* Makes PCA less sensitive to outliers\n",
    "* Applications in video-processing (low-rank + sparse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Matrix completion\n",
    "\n",
    "Given partial knowledge of the entries of $X$, fill in the missing ones assuming a low-rank structure of $X$\n",
    "\n",
    "$$\\min_{W}\\, \\text{rank}(W) \\quad \\text{s.t.} \\quad w_{ij} = x_{ij} \\, \\text{for}\\, (i,j) \\in \\Omega.$$\n",
    "\n",
    "* A famous example is the *Netflix problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/Rank-1-matrix-completion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Auto-encoders\n",
    "\n",
    "* Find an *encoder*, $f$, and *decoder*, $g$ such that $x^{(i)} \\approx g\\circ f(x^{(i)})$ for all samples\n",
    "* $z^{(i)} = f(x^{(i)})$ is the *compressed* sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Matrix factorization gives a linear encoder $f(x) = V^Tx$ and decoder $g(z) = Uz$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Popular auto-encoders are based on Neural Networks:\n",
    "![](./figures/Autoencoder_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges\n",
    "\n",
    "* We may not be able to form the sample covariance matrix $XX^T$\n",
    "* Computing the full SVD may not be computationally feasible\n",
    "* We may have incomplete data\n",
    "* Closed-form solutions may not be available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The rest of the talk\n",
    "\n",
    "* Algorithms for approximating the SVD\n",
    "* Optimization-based formulations\n",
    "* Beyond matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximating the SVD\n",
    "\n",
    "Goal is to decompose a matrix\n",
    "$$X = USV^T,$$\n",
    "with $U$ and $V$ unitary and $S$ a non-negative diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The [traditional approach](https://www.youtube.com/watch?v=R9UoFyqJca8) has a computational complexity of $\\mathcal{O}(mn^2)$ (when $m \\geq n$).\n",
    "* We usually only need a few of the largest singular vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/R9UoFyqJca8?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lanczos / Arnoldi method\n",
    "\n",
    "**Observation:** repeated application of a matrix to a vector will align the vector with the largest eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Start with a random vector $u_0$, compute $u_1 = (XX^T)^ku_0$, yields largest (left-) singular vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Repeat 1 starting from vector ortogonal to $u_1$, yielding $u_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/powers1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/powers2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/powers3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomized SVD\n",
    "**Observation:** distances between points are preserverd under random projection (Johnson-Lindenstrauss Lemma).\n",
    "\n",
    "There is a linear map $P \\in \\mathbb{R}^{k\\times n}$ with $k > 8\\log(m/\\epsilon^2)$ such that \n",
    "$$(1 - \\epsilon)\\|x^{(i)} - x^{(j)}\\|_2 \\leq \\|Px^{(i)} - Px^{(j)}\\|_2 \\leq (1 + \\epsilon)\\|x^{(i)}- x^{(j)}\\|_2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Choose a suitable random matrix $P \\in \\mathbb{R}^{k\\times n}$ with $k = \\mathcal{O}(p)$\n",
    "2. Project all samples $x^{(i)}$ and collect in a matrix $Y \\in \\mathbb{R}^{k\\times m}$\n",
    "3. Form an ortogonal basis, $Q$, for the row-space of $Y$\n",
    "4. Project the data matrix into a small matrix $B = XQ \\in \\mathbb{R}^{n \\times k}$ perform an SVD on $B = U S \\widetilde{V}^T$\n",
    "5. We get $X \\approx U S \\widetilde{V}Q^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison\n",
    "\n",
    "Important factos when choosing a method are\n",
    "* availability of the data (in core, out of core, streaming)\n",
    "* required accuracy\n",
    "* properties of the data (decay of singular values)\n",
    "* need for parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization-based formulations\n",
    "\n",
    "Both covariance estimation and matrix decomposition leads to optimization problems of the form\n",
    "\n",
    "$$\\min_{Z} \\, L(Z) + R(Z),$$\n",
    "\n",
    "* may include the constraint $Z \\succ 0$ (positive definiteness).\n",
    "* $R$ may be non-convex (e.g., $\\text{rank}(Z)$)\n",
    "* $L$ may be non-convex and non-smooth (e.g., $\\text{nnz}(Z)$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic ingredients\n",
    "\n",
    "* manifold optimization\n",
    "* convex relaxation\n",
    "* coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Manifold optimization\n",
    "\n",
    "* encode structure ($Z \\succ 0$, $\\text{rank}(Z) = p$) explicitly\n",
    "* this avoids non-convex or non-smooth penalties\n",
    "* software libraries are available to help implement this (`Manopt`, `McTorch`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Manifold\n",
    "\n",
    "![](./figures/manifolde.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A Manifold\n",
    "\n",
    "![](./figures/Manifoldp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Manifold\n",
    "\n",
    "![](./figures/manifolds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Manifold optimization\n",
    "\n",
    "![](./figures/steepestdescent_compare_euclidean_sphere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple example\n",
    "\n",
    "$$\\min_{x\\in\\mathbb{R}^2}\\, f(x) \\quad \\text{s.t.} \\quad \\|x\\|_2 = 1.$$\n",
    "\n",
    "* Projection onto tangent space is given by $(I - xx^T)$, retraction by $\\|x\\|_2^{-1}I$.\n",
    "* Using a penalty $|\\|x\\|_2 - 1|$ makes the problem non-convex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figures/manifold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Symmetric positive definite matrices\n",
    "![](https://www.convexoptimization.com/wikimization/images/d/d9/Psdcone.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex relaxation\n",
    "\n",
    "The rank is a non-convex penalty, counting the number of non-zero eigenvalues.\n",
    "\n",
    "![](./figures/nuclearnorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many of the programs discussed earlier can now be formulated as\n",
    "\n",
    "$$\\min_X\\, L(X) + \\alpha \\|X\\|_*,$$\n",
    "\n",
    "where $\\|X\\|_*$ denotes the *nuclear norm* which is the sum of the singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Solution coincides with rank minimization in certain cases\n",
    "* $\\|X\\|_*$ requires computing a singular value decomposition\n",
    "* It may not be feasible to store the full matrix $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-convexification (?!)\n",
    "\n",
    "* The nuclear norm may be written as an optimization problem $\\|X\\|_* = \\min_{U,V} \\textstyle{\\frac{1}{2}}\\left(\\|U\\|_F^2 + \\|V\\|_{F}^2\\right) \\quad \\text{s.t.} \\quad X = UV^T$.\n",
    "* This suggests to formulate rank minimization as\n",
    "$$\\min_{U,V}\\, L(UV^T) + \\frac{\\alpha}{2}\\left(\\|U\\|_F^2 + \\|V\\|_{F}^2\\right).$$\n",
    "* Solution coincides with nuclear norm minimization under certain circumstances\n",
    "* Practical algorithms alternate between updating $U$ and $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensors\n",
    "\n",
    "![](./figures/tensors.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/tensor1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/tensor2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Tensors can be used to capture higher order moments of the data\n",
    "* There are a number of ways to define structured decompositions for tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further reading\n",
    "\n",
    "* https://arxiv.org/pdf/0909.4061.pdf\n",
    "* https://arxiv.org/pdf/1505.07570.pdf\n",
    "* https://www.manopt.org/\n",
    "* https://arxiv.org/abs/1810.01811\n",
    "* https://arxiv.org/pdf/1711.10781.pdf\n",
    "* https://epubs.siam.org/doi/abs/10.1137/07070111X"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
