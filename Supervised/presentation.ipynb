{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Inside the black box of Machine Learning I\n",
    "Tristan van Leeuwen, Utrecht University\n",
    "\n",
    "![](cube-250082_1280.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://imgs.xkcd.com/comics/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "* From learning to Optimization\n",
    "* Introduction to Optimization\n",
    "* Some practical aspects\n",
    "* Current trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **From learning to Optimization**\n",
    "* Introduction to Optimization\n",
    "* Some practical aspects\n",
    "* Current trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised learning\n",
    "\n",
    "Given *samples* $x^{(i)}$ and *labels* $y^{(i)}$ find a *function* $f$ such that $f(x^{(i)}) \\approx y^{(i)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* For the purpose of this talk we'll consider $x^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\mathbb{R}$ (regression) or $y^{(i)} \\in \\{-1,1\\}$ (classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ultimately, one would use $f$ for *prediction*, so generalizability is very important. Today, we'll only discuss algorithms for finding $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a line through a set of points\n",
    "\n",
    "![](./figures/interpolate0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a line through a set of points\n",
    "\n",
    "![](./figures/interpolate1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Figure out which papays are yummy\n",
    "\n",
    "![](./figures/papaya0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Figure out which papaya's are yummy\n",
    "\n",
    "![](./figures/papaya1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The supervised learning zoo\n",
    "\n",
    "* Regression\n",
    "* Neural networks\n",
    "* Support Vector Machines\n",
    "* ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Linear regression\n",
    "\n",
    "$$f(x) = w_0 + \\sum_{k=1}^n w_i x_i,$$\n",
    "\n",
    "where the parameters are determined by solving\n",
    "\n",
    "$$\\min_{w} \\sum_{i=1}^m \\left(f(x_i) - y_i\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: binary classification\n",
    "\n",
    "For classification $y_i \\in \\{-1,1\\}$ we take\n",
    "\n",
    "$$f(x) = \\sigma\\bigl(Wx + b\\bigr),$$\n",
    "\n",
    "with $\\sigma(y) = (1 + \\exp(-y))^{-1}$, and determine $w$ by solving\n",
    "\n",
    "$$\\min_{w} \\sum_{i=1}^m \\left(1 - y_if(x_i)\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Neural networks\n",
    "\n",
    "$$f(x) = f_n \\circ f_{n-1} \\circ \\ldots \\circ f_1(x),$$\n",
    "with\n",
    "$$f_k(x) = \\sigma \\left(W_k x + b_k\\right).$$\n",
    "\n",
    "* The architecture is determined by the structure of the matrices $W_k$. \n",
    "* The optimal weights are found by *training*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Support vector machines\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^m w_i k(x_i,x),$$\n",
    "\n",
    "* The kernel $k(x,x')$ determines the properties of the function\n",
    "* The weights are determined by solving\n",
    "$$\\min_{w} L(f(x_i), y_i) + w^T\\!Kw,$$\n",
    "where $k_{ij} = k(x_i, x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* From learning to Optimization\n",
    "* **Introduction to Optimization**\n",
    "* Some practical aspects\n",
    "* Current trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "Find a minimizer of a *cost function*\n",
    "$$C(w) = \\sum_{i=1}^m L(f_i(w),y_i) + R(w),$$\n",
    "\n",
    "where\n",
    "\n",
    "* $w \\in \\mathbb{R}^n$ parametrizes the function\n",
    "* $f_i(w)$ is the predicted label for $x_i$\n",
    "* $L(f(x),y)$ is the *loss function*, and\n",
    "* $R(w)$ is the *regularization term*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: linear regression\n",
    "\n",
    "Let \n",
    "\n",
    "* $f_i(w) = w_0 + \\sum_{k=1}^n w_i x_i$\n",
    "* $L(f_i,y_i) = (f_i - y_i)^2$\n",
    "* $R(w) = \\beta\\|w\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This leads to a least-squares problem \n",
    "\n",
    "$$C(w) = \\|Xw - y\\|_2^2 + \\beta\\|w\\|_2^2,$$\n",
    "\n",
    "with a closed-form solution\n",
    "\n",
    "$$\\widehat{w} = \\left(X^T\\!X + \\beta I\\right)^{-1}X^Ty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "\n",
    "The optimization problem \n",
    "\n",
    "$$\\min_w \\sum_{i=1}^m L(f_i(w),y_i) + R(w),$$\n",
    "\n",
    "* Often does not have a closed-form solution\n",
    "* May not have a *unique* minimizer\n",
    "* Has too many variables to allow for sampling-based methods\n",
    "* Does have a very particular *structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Furthermore:\n",
    "* Evaluation of the cost function may be computationally expensive\n",
    "* Evaluating the gradient may be difficult\n",
    "* A good initial guess of the parameters may not be available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization 101\n",
    "\n",
    "* Structure of the cost function\n",
    "* Optimality conditions\n",
    "* Basic iterative algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss functions\n",
    "\n",
    "Measure the difference between the prediction $f_i$ and the label $y_i$:\n",
    "\n",
    "![](./figures/Loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularizers\n",
    "\n",
    "Promote certain regularity of the coefficients:\n",
    "\n",
    "![](./figures/Regularizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Structure of the cost function\n",
    "\n",
    "Three properties come up frequently in optimization:\n",
    "\n",
    "* Continuity\n",
    "* Convexity\n",
    "* Smoothness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structure of the cost function - Lipschitz continuity\n",
    "\n",
    "A function $C : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is Lipschitz continuous with Lipschitz constant $\\rho$ if $\\forall \\,w,w' \\in \\mathbb{R}^n$\n",
    "\n",
    "$$\\left|C(w) - C(w')\\right| \\leq \\rho \\|w - w'\\|_2.$$\n",
    "\n",
    "![](./figures/Lipschitz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structure of the cost function - convexity\n",
    "\n",
    "A function $C : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is convex if $\\forall w,w' \\in \\mathbb{R}^n$ and $\\forall t \\in [0,1]$ we have\n",
    "\n",
    "$$C(tw + (1-t)w') \\leq tC(w) + (1-t)C(w').$$\n",
    "\n",
    "![](./figures/Convex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structure of the cost function - smoothness\n",
    "\n",
    "A function $C : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is $\\mathcal{C}^k$-smooth if all partial derivatives or order $\\leq k$ exist and are continuous.\n",
    "\n",
    "For $k \\geq 1$:\n",
    "* The Lipschitz constant can be found in terms of the gradient $\\max_{w}\\|\\nabla C(w)\\|$\n",
    "* The Lipschitz constant of the gradient, $\\ell$, is of interest.\n",
    "* The function is $\\mu$-*strongly convex* if $C(w) \\geq C(w') + \\langle \\nabla C(w'),(w - w')\\rangle + \\frac{\\mu}{2}\\|w-w'\\|_2^2$. \n",
    "* The constant $\\ell/\\mu$ is called the *condition number* of $C$.\n",
    "\n",
    "For $k \\geq 2$:\n",
    "* The Lipschitz constant of the *gradient* is the largest eigenvalue of the Hessian\n",
    "* If the Hessian is positive semidefinite, the function is convex\n",
    "* If the Hessian is positive definite, the function is *strongly* convex (with constant $\\mu$ the smallest eigenvalue of the Hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structure of the cost function - strong convexity\n",
    "\n",
    "![](./figures/strongconvex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Structure of the cost function\n",
    "\n",
    "Note:\n",
    "\n",
    "* It is often not possible to find the (global) bounds $(\\ell, \\mu)$ a-priori\n",
    "* The objective may not be globally (strongly) convex, but is usefull to think of these as local properties that hold close to a minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimality conditions - Local and global minima\n",
    "\n",
    "* For convex functions, a local minimum is also a global minimum\n",
    "* For strongly convex functions, the minimum is unique\n",
    "\n",
    "![](./figures/minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimality conditions (for local minima)\n",
    "\n",
    "**Smooth function:**\n",
    "* Gradient is zero, Hessian is positive (semi-)definite\n",
    "\n",
    "**Convex function:**\n",
    "* Subgradient contains zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figures/optimality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithms\n",
    "\n",
    "Basic template (for smooth cost functions)\n",
    "\n",
    "$$w_{k+1} = w_k + \\alpha_k d_k,$$\n",
    " \n",
    "where $\\alpha_k$ is the *stepsize* and the *search direction* is constructed from the gradient:\n",
    "* $d_k = - \\nabla C(w_k)$ (steepest descent)\n",
    "* $d_k = - \\nabla C(w_k) + \\beta_k d_{k-1}$ (conjugate gradients)\n",
    "* $d_k = - B_k \\nabla C(w_k)$ (quasi-Newton)\n",
    "* $d_k = - \\left(\\nabla^2 C(w_k)\\right)^{-1}\\nabla C(w_k)$ (Newton's method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the search direction needs to be descent direction: $\\langle \\nabla C(w_k), d_k \\rangle < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms - steepest descent\n",
    "\n",
    "* for *Lipschitz smooth* $C$:\n",
    "$$\\|\\nabla C(w_k)\\|^2 \\leq c k^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for *convex* $C$\n",
    "$$C(w_k) - C(w_*) \\leq c k^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* for *strongly* convex $C$:\n",
    "$$C(w_{k}) - C(w_*) \\leq \\left(1 - 2\\mu\\alpha_k + \\mu\\ell\\alpha_k^2 \\right)^{k}\\left(C(w_0) - C(w_*)\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms - steepest descent\n",
    "\n",
    "Steepest descent with a fixed stepsize ($\\alpha_k \\in (0,2/\\ell)$):\n",
    "\n",
    "* The iterates converge to a *stationary point* from *any* initial point \n",
    "\n",
    "* For convex functions, the iterates converge to a minimum at a *sub-linear* rate of $\\mathcal{O}(1/k)$\n",
    "\n",
    "* For strongly convex functions, the iterates converge to the minimum at a *linear* rate $\\mathcal{O}(\\rho^{k})$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To reach a point $w_k$ for which $|C(w_k) - C(w_*)| \\leq \\epsilon$ we need\n",
    "\n",
    "* $\\mathcal{O}(\\epsilon^{-1})$ iterations with a sub-linear rate\n",
    "* $\\mathcal{O}(\\log \\epsilon^{-1})$ iterations with a linear rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms - beyond steepest descent\n",
    "\n",
    "* acceleration (conjugate gradient, momentum)\n",
    "* second order methods (quasi-Newton, Gauss-Newton, Newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms - comparison\n",
    "\n",
    "* **Steepest descent:** at best *linear convergence* to stationary point from *any* initial point\n",
    "* **Conjugate gradient / Quasi-newton:** Faster convergence in practice (*superlinear*), less robust\n",
    "* **Full Newton:** *Quadratic convergence* to local minimum when starting nearby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figures/steepdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms - comparison\n",
    "\n",
    "In practice we need to trade-off convergence rates, robustness and computational cost:\n",
    "\n",
    "* Steepest descent has cheap iterations\n",
    "* Quasi-Newton typically requires some storage to build up curvature information\n",
    "* Full Newton requires the solution of a large system of linear equations at each iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithms for non-smooth regularization\n",
    "\n",
    "We can use *sub-gradients*:\n",
    "\n",
    "$$w_{k+1} = w_k - \\alpha_k\\left(\\nabla L(w_k) + \\partial R(w_k) \\right),$$\n",
    "\n",
    "and get a sub-linear convergence rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "...or modify the basic iteration to retain a linear convergence rate when $L$ is strongly convex\n",
    "\n",
    "$$w_{k+1} = \\mathrm{prox}_{\\alpha_k R}\\left(w_k - \\alpha_k\\nabla L(w_k)\\right),$$\n",
    "\n",
    "where $\\text{prox}_{\\alpha R}(z) = \\text{argmin}_x \\, R(x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/proximal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/proxgradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithms for non-smooth regularization\n",
    "\n",
    "Note that when $R$ is the indicator of a convex set, the proximal operator projects onto the convex set.\n",
    "\n",
    "* Box-constraints $w \\in [-1,1]^n$: $\\text{prox}(y) = \\text{sign}(y)\\cdot\\min\\{|y|,1\\}$\n",
    "* 2-norm ball $\\|w\\|_2 \\leq \\tau$: $\\text{prox}(y) = \\tau \\cdot y / \\|y\\|_2$\n",
    "* 1-norm ball $\\|w\\|_1 \\leq \\tau$: $\\text{prox}(y) = \\text{sign}(y)\\cdot \\max \\{|y| - \\tau, 0\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other tricks of the trade\n",
    "\n",
    "* Splitting techniques: $$\\min_{w,v} C(w) + R(v) \\quad \\text{s.t.} \\quad w = v,$$\n",
    "* Smoothing/relaxation $$\\min_{w,v} C(w) + R(v) + \\frac{1}{2\\epsilon}\\|w - v\\|_2^2,$$\n",
    "* Partial minimization $$\\min_{w_1,w_2} C(w_1,w_2) + R_1(w_1) + R_2(w_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* From learning to Optimization\n",
    "* Introduction to Optimization\n",
    "* **Some practical aspects**\n",
    "* Current trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some practical issues\n",
    "\n",
    "* Line search (learning rate)\n",
    "* Bias-variance trade-off\n",
    "* Level-set methods\n",
    "* Vanishing/exploding gradient in RNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Line search\n",
    "\n",
    "* Analysis so far was based on a constant (worst-case) stepsize.\n",
    "* In practice, we can improve performance by choosing a clever stepsize.\n",
    "\n",
    "![](./figures/linesearch1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient decrease\n",
    "\n",
    "![](./figures/linesearch2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Curvature condition\n",
    "\n",
    "![](./figures/linesearch4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-variance trade-off\n",
    "\n",
    "Regularized linear regression:\n",
    "\n",
    "$$\\min_w \\|Xw - y\\|_2^2 + \\beta \\|w\\|_2^2.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figures/biasvariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Choosing an appropriate regularization parameter is its own optimization problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/800/1*MlG-Q6M_hcPTw2h8CLq89A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ill-posedness \n",
    "\n",
    "These examples indicate that the resulting $f$ is very sensitive;\n",
    "\n",
    "$$|f(x) - f(y)| \\leq C |x - y|,$$\n",
    "\n",
    "for very large constant $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Level-set methods\n",
    "\n",
    "When $C$ and $R$ are convex, the following three problems are equivalent\n",
    "\n",
    "$$\\min_{w} C(w) + \\lambda R(w),$$\n",
    "\n",
    "$$\\min_{w} C(w) \\quad \\text{s.t.} \\quad R(w) \\leq \\tau,$$\n",
    "\n",
    "$$\\min_{w} R(w) \\quad \\text{s.t.} \\quad C(w) \\leq \\sigma.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* One formulation may be easier to solve then the other\n",
    "* In terms of regularization, $\\sigma$ or $\\tau$ may be easier to estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The parameters $\\tau$ and $\\sigma$ are connected through the *Pareto curve* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To find the $\\tau$ corresponding to a given $\\sigma$ we solve a rootfinding problem with the *value function*\n",
    "\n",
    "$$\\phi(\\tau) = \\min_{w} C(w) \\quad \\text{s.t.} \\quad R(w) \\leq \\tau.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./figures/pareto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanishing / exploding gradients in RNN's\n",
    "\n",
    "The function has a recursive structure: $$f(x_0) = x_n, \\quad x_k = \\sigma\\left(W_{k-1} x_{k-1}+ b_{k-1}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Computing the gradient (*back-propagation*) requires evaluating the adjoint of the *tangent linear model* \n",
    "\n",
    "$$\\Delta x_{k} = \\sigma'\\left(W_{k-1} x_{k-1}+ b_{k-1}\\right)\\cdot (W_{k-1}\\Delta x_{k-1}).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This dynamical system may not be stable, even if the network is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The design of stable network architecturs is important in many deep-learning applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* From learning to Optimization\n",
    "* Introduction to Optimization\n",
    "* Some practical aspects\n",
    "* **Current trends**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some current trends\n",
    "\n",
    "* Stochastic optimization and acceleration\n",
    "* Visualization/insight\n",
    "* Modelling languages for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic optimization\n",
    "\n",
    "Minimize *expected loss*:\n",
    "\n",
    "$$\\min_w \\mathbb{E} C(w; \\xi).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to evaluate the objective at only one sample at a time:\n",
    "$$w_{k+1} = w_k - \\alpha_k \\nabla C(w_k; x_i),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or use a *batch*\n",
    "$$w_{k+1} = w_k - \\frac{\\alpha_k}{|\\mathcal{I}_k|} \\sum_{i\\in\\mathcal{I}_k}\\nabla C(w_k; x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can interpret this algorithm as gradient-descent on the *full objective* that includes all $m$ training-samples with a noisy gradient: $\\nabla C(w) + E_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batching\n",
    "\n",
    "Full gradient:\n",
    "$$\\nabla C(w) = \\frac{1}{m}\\sum_{i=1}^m \\nabla C(w,x_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "can be approximated using a sample-average\n",
    "$$\\nabla C(w) \\approx \\frac{1}{|\\mathcal{I}|}\\sum_{i=\\in\\mathcal{I}} \\nabla C(w,x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic optimization\n",
    "\n",
    "Main assumptions:\n",
    "\n",
    "* $C$ is strongly convex\n",
    "* Gradient direction is correct on average: $\\mathbb{E}(E_k) = 0$ (e.g., choose $\\mathcal{I}_k$ uniformly at random from $\\{1, 2, \\ldots, m\\}$)\n",
    "* Variance of the error is bounded: $\\mathbb{E}(\\|E_k\\|_2^2)\\leq \\sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Basic iteration produces iterates for which\n",
    "$$\\mathbb{E}(C(w_{k+1}) - C(w_*)) \\leq \\left(1 - 2\\mu\\alpha_k + \\mu\\ell\\alpha_k^2\\right)\\left(C(w_{k}) - C(w_*)\\right) + \\frac{\\alpha_k^2 \\sigma ^2\\ell }{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\mathbb{E}(C(w_{k+1}) - C(w_*)) \\leq \\left(1 - 2\\mu\\alpha_k + \\mu\\ell\\alpha_k^2\\right)\\left(C(w_{k}) - C(w_*)\\right) + \\frac{\\alpha_k^2 \\sigma ^2 \\ell}{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic optimization\n",
    "\n",
    "We find\n",
    "* linear convergence to a point close to a minimizer with fixed stepsize\n",
    "* sublinear convergence to the minimizer when using a diminishing stepsize $\\alpha_k = 1/k$.\n",
    "\n",
    "![](./figures/stochastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acceleration\n",
    "\n",
    "Linear convergence without stepsize reduction can be retained by *variance reduction*:\n",
    "\n",
    "* Increase *batchsize*\n",
    "* Use previous gradients (SVRG, SAGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Further improvements:\n",
    "* Constants can be improved by gradient-scaling\n",
    "* Accelaration for non-smooth terms using proximal algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Project iterates onto principal components of\n",
    "\n",
    "$$M = \\left(w_0 - w_n, w_{1} - w_{n}, \\ldots, w_{n-1} - w_n\\right),$$\n",
    "\n",
    "and plot objective along dominant directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figures/visual.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-26-at-10.50.53-PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelling languages\n",
    "\n",
    "* http://www.pyomo.org/\n",
    "* http://www.pyopt.org/\n",
    "* http://cvxopt.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "\n",
    "* Supervised learning leads to interesting and structured optimization problems\n",
    "* Many specialized algorithms are available to take advantage of this structure\n",
    "* Stochastic methods can be usefully analyzed as gradient-descent-with-errors\n",
    "* Exciting new research in (applied) math!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further reading\n",
    "\n",
    "* [Extensive overview of optimization methods](http://ruder.io/optimizing-gradient-descent/)\n",
    "* [Another extensive overview](https://www.researchgate.net/profile/Frank_E_Curtis/publication/303992986_Optimization_Methods_for_Large-Scale_Machine_Learning/links/5866da5908aebf17d39aeba7/Optimization-Methods-for-Large-Scale-Machine-Learning.pdf)\n",
    "* [The effect of regularization in classification](https://thomas-tanay.github.io/post--L2-regularization/)\n",
    "* [Well-posed network architectures](https://arxiv.org/pdf/1705.03341.pdf)\n",
    "* [Visualizing the loss](https://arxiv.org/pdf/1712.09913.pdf)\n",
    "* [Partial minimization](https://arxiv.org/abs/1601.05011)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
